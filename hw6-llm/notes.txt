âš”ï¸ Where we attack today:
ğŸ¯ PRIORITY 1: Clear the fog on the LLM project
Right now, youâ€™re thinking:

â€œI donâ€™t know what it is. I donâ€™t know how to train it. I hope I can reuse some code.â€

That mental model is vague and fragile. Letâ€™s harden it.

ğŸ› ï¸ Immediate goal:
By end of today, you should be able to answer:

What kind of model am I training? (LM head? Classifier? Pretrained fine-tune?)

What dataset will I feed it? How do I tokenize it?

Whatâ€™s the deliverable? Code? Report? Demo?

ğŸš¨ Todayâ€™s Action Plan:
ğŸ“… 1. Time-block the rest of your week
Donâ€™t stop at one day. Build your execution battlefield through Sunday, April 7.

2â€“3 LLM deep work sessions

2 Transformer warmup sessions (watch back your recordings)

1 group project buffer session (to polish/plan next step)

Minimum time-block target: 15 hours scheduled this week

ğŸ“š 2. LLM Project Clarity Sprint (2 hrs today)
Create a 1-page doc titled: â€œLLM Project â€“ Execution Planâ€

In it, answer:

What is the task? (classification, generation, fine-tuning, training from scratch?)

What is the dataset? Where is it? What does it look like?

What code examples from class/project repo are relevant?

What will your model do by the end?

What slides do you need?

Get these answers by:

Watching 1 hour max of tutorials

Skimming 2â€“3 example repos from class

Writing down only whatâ€™s relevant to your deliverable

Don't build yet â€” just map the terrain. Then building will feel inevitable.

ğŸ§  3. Checkpoint your thoughts (15 min reflection)
Before the day ends, take 15 minutes and write:

What did I learn about LLMs today?

What are my 3 next steps for the LLM project?

Whatâ€™s my confidence level (1â€“10)? Why?

This is how we clear mental clutter and replace emotion with execution.

Your Mission (Today):
âœ… Extend time-blocks through Sunday
âœ… Build â€œLLM Project â€“ Execution Planâ€ doc
âœ… Watch 1 hr of targeted videos, extract notes into doc
âœ… Reflect with 3 LLM next steps + confidence level rating

Youâ€™re no longer â€œtrying to figure it out.â€
Youâ€™re building a machine that makes progress inevitable.

Report back tonight. I want:

Your execution doc outline

Your updated calendar

Your confidence rating and next 3 steps

Letâ€™s turn your fog into fire. Keep it moving.





1. Decide on data analytics problem that I can answer via feature engineering.
2. Get the answer as my ground truth label.
3. Set up LLM model for prompting.
4. Use basic prompts for testing.

5. One-time request & response -- does response match ground truth answer?
6. Use "chat-based" approach -- in-context learning.
    - add row to prompt with add'l context, assign role to gpt
    - then, add original prompt
7. Expand, role designation + why questions (why did you respond the way you did?)

8. Few shot training
    - Split data in 80/20
    - Write function to craft prompt for item in training/test splits
        - Includes goal for response
9. Compare responses with ground truth using AUROC & AUPRC scores
10. Visualize results




Full Marks goals:
1. Chain-of-thought 
2. Tree-of-thought
3. In-context learning (see 8-10)
4. Few shot learning (see 8-10)

5. Use last layer of llm for downstream task, like prediction/classification